# -*- coding: utf-8 -*-
"""VIJAY WFH TASK 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15qzkXgWt4-iTDdSbwg-usttRguugCzpY
"""

# Step 1: Install required libraries (run once)
!pip install -q pandas scikit-learn nltk textblob openpyxl

# Step 2: Import libraries
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import scipy.sparse as sp
from sklearn.metrics import classification_report
import json

# Step 3: Download nltk data (run once)
nltk.download('stopwords')
nltk.download('wordnet')

# Step 4: Upload Excel file in Colab
from google.colab import files
uploaded = files.upload()

# Step 5: Load Excel file (filename auto-detected)
file_name = list(uploaded.keys())[0]
df = pd.read_excel(file_name)

print("File loaded. Sample data:")
print(df.head())

# Step 6: Handle missing text and labels
df['ticket_text'] = df['ticket_text'].fillna("")
df = df.dropna(subset=['issue_type', 'urgency_level'])  # drop rows with missing labels

# Step 7: Preprocessing function
def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(tokens)

df['clean_text'] = df['ticket_text'].apply(preprocess)

# Step 8: Feature engineering - TF-IDF + length + sentiment
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df['clean_text'])

df['ticket_len'] = df['ticket_text'].apply(len)
df['sentiment'] = df['ticket_text'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Fill any NaN in numeric features with 0
df['ticket_len'] = df['ticket_len'].fillna(0)
df['sentiment'] = df['sentiment'].fillna(0)

X_extra = sp.csr_matrix(df[['ticket_len', 'sentiment']].values)
X = sp.hstack([X_tfidf, X_extra])

# Step 9: Labels
y_issue = df['issue_type']
y_urgency = df['urgency_level']

# Step 10: Split data
X_train, X_test, y_issue_train, y_issue_test, y_urgency_train, y_urgency_test = train_test_split(
    X, y_issue, y_urgency, test_size=0.2, random_state=42)

# Step 11: Train Logistic Regression models
model_issue = LogisticRegression(max_iter=300)
model_issue.fit(X_train, y_issue_train)

model_urgency = LogisticRegression(max_iter=300)
model_urgency.fit(X_train, y_urgency_train)

# Step 12: Entity extraction rules
product_list = df['product'].dropna().unique().tolist()
complaint_keywords = ['broken', 'late', 'error', 'fail', 'delay', 'problem', 'issue', 'refund']

def extract_entities(text):
    entities = {}
    text_lower = text.lower()
    entities['products'] = [p for p in product_list if p.lower() in text_lower]
    date_pattern = r'\b(?:\d{1,2}[/-]\d{1,2}[/-]\d{2,4}|\d{4}[/-]\d{1,2}[/-]\d{1,2})\b'
    entities['dates'] = re.findall(date_pattern, text)
    entities['complaints'] = [kw for kw in complaint_keywords if kw in text_lower]
    return entities

# Step 13: Predict + extract function
def predict_ticket(ticket_text):
    processed = preprocess(ticket_text)
    vect = tfidf.transform([processed])
    length = len(ticket_text)
    sentiment = TextBlob(ticket_text).sentiment.polarity
    features = sp.hstack([vect, sp.csr_matrix([[length, sentiment]])])
    issue_pred = model_issue.predict(features)[0]
    urgency_pred = model_urgency.predict(features)[0]
    entities = extract_entities(ticket_text)
    return {
        "issue_type": issue_pred,
        "urgency_level": urgency_pred,
        "entities": entities
    }

# Step 14: Test on a sample ticket from dataset
sample_text = df['ticket_text'].iloc[0]
print("\nSample Ticket Text:\n", sample_text)
print("\nPrediction and Entities:")
print(json.dumps(predict_ticket(sample_text), indent=2))

# Step 15: Classification reports
print("\nIssue Type Classification Report:")
print(classification_report(y_issue_test, model_issue.predict(X_test)))

print("\nUrgency Level Classification Report:")
print(classification_report(y_urgency_test, model_urgency.predict(X_test)))

!pip install gradio

import gradio as gr

def gradio_predict(text):
    result = predict_ticket(text)
    return f"Issue Type: {result['issue_type']}\nUrgency Level: {result['urgency_level']}\nEntities: {result['entities']}"

iface = gr.Interface(fn=gradio_predict,
                     inputs="text",
                     outputs="text",
                     title="Customer Ticket Classifier & Entity Extractor")

iface.launch()