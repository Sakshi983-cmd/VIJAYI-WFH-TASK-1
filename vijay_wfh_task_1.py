# -*- coding: utf-8 -*-
"""VIJAY WFH TASK 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15qzkXgWt4-iTDdSbwg-usttRguugCzpY
"""

from google.colab import files
import pandas as pd

# Upload the file manually
uploaded = files.upload()  # This will prompt you to select the file

# Get the uploaded file name dynamically
file_name = next(iter(uploaded))

# Load the Excel file into a Pandas DataFrame
df = pd.read_excel(file_name)

# Display the first few rows
df.head()

import pandas as pd

# Load the uploaded file
df = pd.read_excel("ai_dev_assignment_tickets_complex_1000.xls")

# Inspect basic details
df.info()
df.head()

# Fill missing urgency levels with "Unknown"
df['urgency_level'].fillna("Unknown", inplace=True)

# Drop rows with completely missing values
df.dropna(inplace=True)

# Verify again
print(df.isnull().sum())

!pip install nltk
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Initialize tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Define preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

# Apply preprocessing
df['cleaned_ticket_text'] = df['ticket_text'].apply(preprocess_text)

# Check processed data
df[['ticket_text', 'cleaned_ticket_text']].head()

import nltk
print(nltk.data.path)

nltk.data.path.append('/usr/local/share/nltk_data')

import nltk

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')  # Support for wordnet functionalities

import os
print(os.listdir("/usr/local/share/nltk_data/corpora"))

import os
import nltk

# Manually create the missing directory
os.makedirs('/usr/local/share/nltk_data/corpora', exist_ok=True)

# Ensure NLTK looks in the correct location
nltk.data.path.append('/usr/local/share/nltk_data')

# Now download resources properly
nltk.download('stopwords', download_dir='/usr/local/share/nltk_data')
nltk.download('punkt', download_dir='/usr/local/share/nltk_data')
nltk.download('wordnet', download_dir='/usr/local/share/nltk_data')
nltk.download('omw-1.4', download_dir='/usr/local/share/nltk_data')

print("All necessary NLTK resources downloaded successfully!")

# ✅ Step 1: File Upload, Data Inspection, and Text Preprocessing
from google.colab import files
import pandas as pd
import os
import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# --- 1️⃣ Upload & Load the File ---
uploaded = files.upload()  # Upload Excel file manually
file_name = next(iter(uploaded))  # Get the uploaded file name
df = pd.read_excel(file_name)  # Load the Excel file

# --- 2️⃣ Verify File Upload ---
print("Files in Colab environment:", os.listdir())  # Confirm file is available

# --- 3️⃣ Inspect Data Structure ---
df.info()  # Check columns & data types
df.head()  # Preview first few rows

# --- 4️⃣ Handle Missing Values ---
df['urgency_level'].fillna("Unknown", inplace=True)  # Fill missing urgency labels
df.dropna(inplace=True)  # Drop rows with missing values (optional)

# --- 5️⃣ Ensure NLTK Resources Are Installed ---
os.makedirs('/usr/local/share/nltk_data/corpora', exist_ok=True)  # Create missing directory
nltk.data.path.append('/usr/local/share/nltk_data')  # Set correct NLTK data path

nltk.download('stopwords', download_dir='/usr/local/share/nltk_data')
nltk.download('punkt', download_dir='/usr/local/share/nltk_data')
nltk.download('wordnet', download_dir='/usr/local/share/nltk_data')
nltk.download('omw-1.4', download_dir='/usr/local/share/nltk_data')

print("All necessary NLTK resources downloaded successfully!")

# --- 6️⃣ Text Preprocessing ---
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    tokens = word_tokenize(text)  # Tokenize text
    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize words
    return " ".join(tokens)

df['cleaned_ticket_text'] = df['ticket_text'].apply(preprocess_text)  # Apply cleaning

# --- 7️⃣ Confirm Processed Text ---
df[['ticket_text', 'cleaned_ticket_text']].head()

from sklearn.feature_extraction.text import TfidfVectorizer
from textblob import TextBlob

# --- TF-IDF Vectorization ---
vectorizer = TfidfVectorizer(max_features=500)
X_tfidf = vectorizer.fit_transform(df["cleaned_ticket_text"])

# --- Additional Features ---
df["ticket_length"] = df["cleaned_ticket_text"].apply(lambda x: len(x.split()))  # Length of ticket
df["sentiment_score"] = df["cleaned_ticket_text"].apply(lambda x: TextBlob(x).sentiment.polarity)  # Sentiment score

# Merge features
X_features = pd.concat([pd.DataFrame(X_tfidf.toarray()), df[["ticket_length", "sentiment_score"]]], axis=1)

# --- Labels for Classification ---
y_issue = df["issue_type"]
y_urgency = df["urgency_level"]

# Verify data
print("Feature matrix shape:", X_features.shape)
df[["ticket_length", "sentiment_score"]].head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# --- 1️⃣ Split Data for Issue Type Classification ---
X_train_issue, X_test_issue, y_train_issue, y_test_issue = train_test_split(X_features, y_issue, test_size=0.2, random_state=42)

# --- 2️⃣ Train Issue Type Classifier ---
issue_clf = RandomForestClassifier()
issue_clf.fit(X_train_issue, y_train_issue)

# --- 3️⃣ Evaluate Issue Type Classifier ---
y_pred_issue = issue_clf.predict(X_test_issue)
print("Issue Type Accuracy:", accuracy_score(y_test_issue, y_pred_issue))

# --- 4️⃣ Split Data for Urgency Classification ---
X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X_features, y_urgency, test_size=0.2, random_state=42)

# --- 5️⃣ Train Urgency Level Classifier ---
urgency_clf = RandomForestClassifier()
urgency_clf.fit(X_train_urgency, y_train_urgency)

# --- 6️⃣ Evaluate Urgency Level Classifier ---
y_pred_urgency = urgency_clf.predict(X_test_urgency)
print("Urgency Level Accuracy:", accuracy_score(y_test_urgency, y_pred_urgency))

# Reset indices to avoid misalignment
X_features = X_features.reset_index(drop=True)
y_issue = y_issue.reset_index(drop=True)
y_urgency = y_urgency.reset_index(drop=True)

# Ensure no mismatch in dataset sizes
X_features = X_features.loc[y_issue.index]

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# --- 1️⃣ Split Data for Issue Type Classification ---
X_train_issue, X_test_issue, y_train_issue, y_test_issue = train_test_split(X_features, y_issue, test_size=0.2, random_state=42)

# --- 2️⃣ Train Issue Type Classifier ---
issue_clf = RandomForestClassifier()
issue_clf.fit(X_train_issue, y_train_issue)

# --- 3️⃣ Evaluate Issue Type Classifier ---
y_pred_issue = issue_clf.predict(X_test_issue)
print("Issue Type Accuracy:", accuracy_score(y_test_issue, y_pred_issue))

# --- 4️⃣ Split Data for Urgency Classification ---
X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X_features, y_urgency, test_size=0.2, random_state=42)

# --- 5️⃣ Train Urgency Level Classifier ---
urgency_clf = RandomForestClassifier()
urgency_clf.fit(X_train_urgency, y_train_urgency)

# --- 6️⃣ Evaluate Urgency Level Classifier ---
y_pred_urgency = urgency_clf.predict(X_test_urgency)
print("Urgency Level Accuracy:", accuracy_score(y_test_urgency, y_pred_urgency))

# Ensure all column names are strings
X_features.columns = X_features.columns.astype(str)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# --- Split Data ---
X_train_issue, X_test_issue, y_train_issue, y_test_issue = train_test_split(X_features, y_issue, test_size=0.2, random_state=42)

# --- Train Issue Type Classifier ---
issue_clf = RandomForestClassifier()
issue_clf.fit(X_train_issue, y_train_issue)

# --- Evaluate Issue Type Classifier ---
y_pred_issue = issue_clf.predict(X_test_issue)
print("Issue Type Accuracy:", accuracy_score(y_test_issue, y_pred_issue))

# --- Split Data for Urgency Classification ---
X_train_urgency, X_test_urgency, y_train_urgency, y_test_urgency = train_test_split(X_features, y_urgency, test_size=0.2, random_state=42)

# --- Train Urgency Level Classifier ---
urgency_clf = RandomForestClassifier()
urgency_clf.fit(X_train_urgency, y_train_urgency)

# --- Evaluate Urgency Level Classifier ---
y_pred_urgency = urgency_clf.predict(X_test_urgency)
print("Urgency Level Accuracy:", accuracy_score(y_test_urgency, y_pred_urgency))

!pip install spacy
import spacy

# Load pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

def extract_entities(text):
    doc = nlp(text)
    entities = {"product": [], "date": [], "keywords": []}

    # Extract Named Entities
    for ent in doc.ents:
        if ent.label_ == "PRODUCT":
            entities["product"].append(ent.text)
        elif ent.label_ == "DATE":
            entities["date"].append(ent.text)

    # Extract complaint keywords manually
    keywords = ["broken", "late", "error", "failed"]
    words = text.lower().split()
    entities["keywords"] = [word for word in words if word in keywords]

    return entities

# Apply entity extraction to ticket text
df["extracted_entities"] = df["ticket_text"].apply(extract_entities)

# Check results
df[["ticket_text", "extracted_entities"]].head()

def classify_ticket(ticket_text):
    # Preprocess text
    processed_text = preprocess_text(ticket_text)

    # Vectorize input
    vectorized_text = vectorizer.transform([processed_text]).toarray()

    # Extract additional features
    length_feature = [[len(processed_text.split())]]
    sentiment_feature = [[TextBlob(processed_text).sentiment.polarity]]

    # Merge features
    final_features = pd.concat([pd.DataFrame(vectorized_text), pd.DataFrame(length_feature), pd.DataFrame(sentiment_feature)], axis=1)

    # Predict issue type and urgency level
    issue_pred = issue_clf.predict(final_features)[0]
    urgency_pred = urgency_clf.predict(final_features)[0]

    # Extract entities
    extracted_entities = extract_entities(ticket_text)

    return {
        "Predicted Issue Type": issue_pred,
        "Predicted Urgency Level": urgency_pred,
        "Extracted Entities": extracted_entities
    }

# Test the function with an example ticket
sample_ticket = "My Vision LED TV arrived 13 days late, and now it won't turn on!"
print(classify_ticket(sample_ticket))

import gradio as gr

def gradio_function(ticket_text):
    result = classify_ticket(ticket_text)
    return result

iface = gr.Interface(
    fn=gradio_function,
    inputs="text",
    outputs="json",
    title="Customer Support Ticket Classifier",
    description="Enter a ticket text to get predictions and entity extraction.",
)

iface.launch()